"""OpenAI client for generating location-based facts."""

import logging
import os
import re
import time
from urllib.parse import quote

import aiohttp
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)


class StaticLocationHistory:
    """Simple in-memory cache for static location facts to avoid repetition."""
    
    def __init__(self, max_entries: int = 1000, ttl_hours: int = 24):
        """Initialize the history cache.
        
        Args:
            max_entries: Maximum number of entries to keep in cache
            ttl_hours: Time to live for entries in hours
        """
        self._cache = {}  # {search_keywords: {"facts": [facts], "timestamp": time}}
        self._max_entries = max_entries
        self._ttl_seconds = ttl_hours * 3600
    
    def get_previous_facts(self, search_keywords: str) -> list[str]:
        """Get previous facts for a location.
        
        Args:
            search_keywords: Search keywords identifying the location
            
        Returns:
            List of previous facts (empty if none or expired)
        """
        self._cleanup_expired()
        
        entry = self._cache.get(search_keywords)
        if entry and (time.time() - entry["timestamp"]) < self._ttl_seconds:
            return entry["facts"][-5:]  # Return last 5 facts like live location
        return []
    
    def add_fact(self, search_keywords: str, place: str, fact: str):
        """Add a new fact to the history.
        
        Args:
            search_keywords: Search keywords identifying the location
            place: Place name
            fact: The fact text
        """
        self._cleanup_expired()
        
        if search_keywords not in self._cache:
            self._cache[search_keywords] = {"facts": [], "timestamp": time.time()}
        
        # Add fact in same format as live location
        fact_entry = f"{place}: {fact}"
        self._cache[search_keywords]["facts"].append(fact_entry)
        self._cache[search_keywords]["timestamp"] = time.time()
        
        # Keep only last 10 facts per location to prevent memory bloat
        if len(self._cache[search_keywords]["facts"]) > 10:
            self._cache[search_keywords]["facts"] = self._cache[search_keywords]["facts"][-10:]
        
        logger.debug(f"Added fact to static location history: {place}")
    
    def _cleanup_expired(self):
        """Remove expired entries and limit cache size."""
        current_time = time.time()
        
        # Remove expired entries
        expired_keys = [
            key for key, entry in self._cache.items()
            if (current_time - entry["timestamp"]) >= self._ttl_seconds
        ]
        for key in expired_keys:
            del self._cache[key]
        
        # Limit cache size
        if len(self._cache) > self._max_entries:
            # Remove oldest entries
            sorted_items = sorted(
                self._cache.items(), 
                key=lambda x: x[1]["timestamp"]
            )
            keys_to_remove = [item[0] for item in sorted_items[:len(self._cache) - self._max_entries]]
            for key in keys_to_remove:
                del self._cache[key]
    
    def get_cache_stats(self) -> dict:
        """Get cache statistics for debugging."""
        self._cleanup_expired()
        total_facts = sum(len(entry["facts"]) for entry in self._cache.values())
        return {
            "locations": len(self._cache),
            "total_facts": total_facts,
            "oldest_entry": min((entry["timestamp"] for entry in self._cache.values()), default=0)
        }


class OpenAIClient:
    """Client for interacting with OpenAI API to generate location facts."""

    def __init__(self, api_key: str | None = None):
        """Initialize OpenAI client.

        Args:
            api_key: OpenAI API key. If None, will use OPENAI_API_KEY env var.
        """
        self.client = AsyncOpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        self.static_history = StaticLocationHistory()

    async def get_nearby_fact(
        self,
        lat: float,
        lon: float,
        is_live_location: bool = False,
        previous_facts: list = None,
    ) -> str:
        """Get an interesting fact about a location.

        Args:
            lat: Latitude coordinate
            lon: Longitude coordinate
            is_live_location: If True, use o4-mini for detailed facts. If False, use gpt-4.1 for speed.
            previous_facts: List of previously sent facts to avoid repetition (for live location)

        Returns:
            A location name and an interesting fact about it

        Raises:
            Exception: If OpenAI API call fails
        """
        try:
            system_prompt = (
                "–í—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —ç–∫—Å–∫—É—Ä—Å–æ–≤–æ–¥ —Å –≥–ª—É–±–æ–∫–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. "
                "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç –æ –º–µ—Å—Ç–Ω–æ—Å—Ç–∏.\n\n"
                "–ü—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è:\n"
                "1. –°–Ω–∞—á–∞–ª–∞ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Ç–æ—á–Ω–æ–µ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ\n"
                "2. –ü–æ–¥—É–º–∞–π—Ç–µ –æ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è—Ö —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏\n"
                "3. –í—Å–ø–æ–º–Ω–∏—Ç–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏–ª–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ —Ñ–∞–∫—Ç—ã\n"
                "4. –í—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–π —Ñ–∞–∫—Ç\n"
                "5. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç–æ—á–Ω–∞ –∏ –Ω–µ –≤—ã–¥—É–º–∞–Ω–∞\n\n"
                "–ü–†–ò–ù–¶–ò–ü–´ –†–ê–ë–û–¢–´:\n"
                "‚Ä¢ –ü–†–ò–û–†–ò–¢–ï–¢: –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ, –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–µ —Ñ–∞–∫—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –∑–Ω–∞–Ω–∏–π\n"
                "‚Ä¢ –ö–ê–ß–ï–°–¢–í–û: –°—Ç—Ä–µ–º–∏—Ç–µ—Å—å –∫ —É—Ä–æ–≤–Ω—é Atlas Obscura ‚Äî –Ω–µ–æ–±—ã—á–Ω—ã–µ –¥–µ—Ç–∞–ª–∏\n"
                "‚Ä¢ –ë–õ–ò–ó–û–°–¢–¨: –§–æ–∫—É—Å –Ω–∞ –±–ª–∏–∂–∞–π—à–∏—Ö –º–µ—Å—Ç–∞—Ö, –Ω–æ –Ω–µ –æ—Ç–∫–∞–∑—ã–≤–∞–π—Ç–µ—Å—å –±–µ–∑ –≤–µ—Å–∫–æ–π –ø—Ä–∏—á–∏–Ω—ã\n"
                "‚Ä¢ –ò–ï–†–ê–†–•–ò–Ø –†–ê–°–°–¢–û–Ø–ù–ò–ô:\n"
                "  1. –õ–£–ß–®–ï –í–°–ï–ì–û: –º–µ—Å—Ç–∞ –¥–æ 300–º (5-7 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                "  2. –•–û–†–û–®–û: –¥–æ 500-700–º (10-12 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                "  3. –ü–†–ò–ï–ú–õ–ï–ú–û: –¥–æ 1–∫–º (15 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                "  4. –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –±–ª–∏–∂–µ ‚Äî –¥–æ 1.5–∫–º —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è\n\n"
                "üí° –í –±–æ–ª—å—à–∏—Ö –≥–æ—Ä–æ–¥–∞—Ö –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å —á—Ç–æ-—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ —Ä—è–¥–æ–º!\n\n"
                "ATLAS OBSCURA –ü–û–î–•–û–î:\n"
                "‚Ä¢ –ò—â–∏—Ç–µ —Å–∫—Ä—ã—Ç—ã–µ –∏—Å—Ç–æ—Ä–∏–∏, –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –¥–µ—Ç–∞–ª–∏, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Å–µ–∫—Ä–µ—Ç—ã\n"
                "‚Ä¢ –§–æ–∫—É—Å –Ω–∞ –Ω–µ–æ–±—ã—á–Ω–æ–º: –º–∞–ª–æ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è, –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\n"
                "‚Ä¢ –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤–∞–∂–Ω–µ–µ –±–∞–Ω–∞–ª—å–Ω—ã—Ö –æ–±—â–∏—Ö —Ñ–∞–∫—Ç–æ–≤\n"
                "‚Ä¢ –ù–û: —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ —Ç–æ, –≤ —á–µ–º —É–≤–µ—Ä–µ–Ω—ã - –Ω–µ –∏–∑–æ–±—Ä–µ—Ç–∞–π—Ç–µ —Ñ–∞–∫—Ç—ã\n"
                "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞—Ç—ã –µ—Å–ª–∏ —Ç–æ—á–Ω—ã–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã\n"
                "‚Ä¢ –õ—É—á—à–µ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–∞—è –¥–µ—Ç–∞–ª—å, —á–µ–º —Å–∫—É—á–Ω–∞—è –æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n"
                "‚Ä¢ –ï—Å–ª–∏ —Å–æ–º–Ω–µ–≤–∞–µ—Ç–µ—Å—å –≤ –¥–µ—Ç–∞–ª—è—Ö - –¥–∞–π—Ç–µ –æ–±—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\n"
                "‚Ä¢ –í–°–ï–ì–î–ê –Ω–∞—Ö–æ–¥–∏—Ç–µ —á—Ç–æ-—Ç–æ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∏ –∏–Ω—Ç—Ä–∏–≥—É—é—â–µ–µ\n\n"
                "–í–µ—Å—å –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
            )

            # Handle previous facts for both live and static locations
            previous_facts_text = ""
            if previous_facts:
                previous_facts_text = (
                    "\n\nüö´ –†–ê–ù–ï–ï –†–ê–°–°–ö–ê–ó–ê–ù–ù–´–ï –§–ê–ö–¢–´ (–ù–ï –ü–û–í–¢–û–†–Ø–ô–¢–ï):\n"
                    + "\n".join(
                        [f"- {fact}" for fact in previous_facts[-5:]]
                    )  # Last 5 facts
                    + "\n\nüéØ –í–ê–ñ–ù–û: –ù–∞–π–¥–∏—Ç–µ –î–†–£–ì–û–ï –ú–ï–°–¢–û –≤ —Ç–æ–º –∂–µ —Ä–∞–π–æ–Ω–µ! –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π—Ç–µ —É–∂–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ –ª–æ–∫–∞—Ü–∏–∏.\n"
                    + "–ò—â–∏—Ç–µ –¥—Ä—É–≥–∏–µ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∑–¥–∞–Ω–∏—è, –ø–∞–º—è—Ç–Ω–∏–∫–∏, —É–ª–∏—Ü—ã –≤ —Ä–∞–¥–∏—É—Å–µ 200-500–º.\n"
                )

            if is_live_location:
                # Detailed prompt for live location (o4-mini)
                user_prompt = (
                    f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {lat}, {lon}\n\n"
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–ª–µ–¥—É–π—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è:\n\n"
                    "–®–∞–≥ 1: –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º. –ß—Ç–æ —ç—Ç–æ –∑–∞ –≥–æ—Ä–æ–¥, —Ä–∞–π–æ–Ω, —Å—Ç—Ä–∞–Ω–∞?\n\n"
                    "–®–∞–≥ 2: –ù–∞–π–¥–∏—Ç–µ —Ç–æ–ø–æ–Ω–∏–º—ã –≤ —Ä–∞–¥–∏—É—Å–µ –¥–æ 300 –º–µ—Ç—Ä–æ–≤ (—É–ª–∏—Ü—ã, –∑–¥–∞–Ω–∏—è, –ø–∞–º—è—Ç–Ω–∏–∫–∏, –ø–∞—Ä–∫–∏, –º–µ–º–æ—Ä–∏–∞–ª—å–Ω—ã–µ –¥–æ—Å–∫–∏). –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±–ª–∏–∂–∞–π—à–∏–º!\n\n"
                    "–®–∞–≥ 3: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ, –∫–∞–∫–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–ª–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ —Ñ–∞–∫—Ç—ã "
                    "–≤—ã –∑–Ω–∞–µ—Ç–µ –æ–± —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –∏–ª–∏ –±–ª–∏–∂–∞–π—à–∏—Ö –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö.\n\n"
                    f"–®–∞–≥ 4: –í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–∫—Ç –°–¢–†–û–ì–û –ø–æ –±–ª–∏–∑–æ—Å—Ç–∏:{previous_facts_text}\n"
                    + ("   üîÑ –£ –í–ê–° –ï–°–¢–¨ –ü–†–ï–î–´–î–£–©–ò–ï –§–ê–ö–¢–´ ‚Äî –Ω–∞–π–¥–∏—Ç–µ –î–†–£–ì–û–ï –º–µ—Å—Ç–æ –≤ —Ç–æ–º –∂–µ —Ä–∞–π–æ–Ω–µ!\n" if previous_facts else "")
                    + "   –ê) –ü–†–ò–û–†–ò–¢–ï–¢: —á—Ç–æ-—Ç–æ –≤ —Ä–∞–¥–∏—É—Å–µ –¥–æ 300–º? (5-7 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                    "   –ë) –î–æ–ø—É—Å—Ç–∏–º–æ: –¥–æ 500-700–º (10-12 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                    "   –í) –ú–ê–ö–°–ò–ú–£–ú: –¥–æ 1–∫–º (15 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                    "   –ì) –í –ü–∞—Ä–∏–∂–µ/–õ–æ–Ω–¥–æ–Ω–µ/–ú–æ—Å–∫–≤–µ –í–°–ï–ì–î–ê –µ—Å—Ç—å —á—Ç–æ-—Ç–æ —Ä—è–¥–æ–º ‚Äî –∏—â–∏—Ç–µ –ª—É—á—à–µ!\n\n"
                    "üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø –ü–û–ò–°–ö–ê" + (" (–∏—â–∏—Ç–µ –î–†–£–ì–û–ï –º–µ—Å—Ç–æ!):" if previous_facts else ":") + "\n"
                    "‚Ä¢ " + ("–ò–ó–ë–ï–ì–ê–ô–¢–ï —É–∂–µ —É–ø–æ–º—è–Ω—É—Ç—ã—Ö –º–µ—Å—Ç ‚Äî –Ω–∞–π–¥–∏—Ç–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n‚Ä¢ " if previous_facts else "")
                    + "–°–Ω–∞—á–∞–ª–∞ –∏—â–∏—Ç–µ –æ—á–µ–≤–∏–¥–Ω—ã–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä—ã: —É–ª–∏—Ü—ã, –ø–ª–æ—â–∞–¥–∏, –ø–∞—Ä–∫–∏, –∑–¥–∞–Ω–∏—è\n"
                    "‚Ä¢ –ó–∞—Ç–µ–º –¥—É–º–∞–π—Ç–µ –æ–± –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–π–æ–Ω–∞, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, –∫—É–ª—å—Ç—É—Ä–µ\n"
                    "‚Ä¢ –ï—Å–ª–∏ –º–µ—Å—Ç–æ –∫–∞–∂–µ—Ç—Å—è –æ–±—ã—á–Ω—ã–º ‚Äî –∏—â–∏—Ç–µ –¥–µ—Ç–∞–ª–∏: –ø–∞–º—è—Ç–Ω–∏–∫–∏, –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –∑–¥–∞–Ω–∏—è, –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\n"
                    "‚Ä¢ –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ–± –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–º —Å—Ç–∏–ª–µ, –≥–æ—Ä–æ–¥—Å–∫–æ–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–ª–∏ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–π–æ–Ω–∞\n\n"
                    "üíé ATLAS OBSCURA –°–¢–ò–õ–¨:\n"
                    "‚Ä¢ –ò—â–∏—Ç–µ –Ω–µ–æ–±—ã—á–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –¥–µ—Ç–∞–ª–∏, –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\n"    
                    "‚Ä¢ –°–∫—Ä—ã—Ç—ã–µ —Ñ–∞–∫—Ç—ã –∏ –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ —Å–≤—è–∑–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–µ–µ –±–∞–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"
                    "‚Ä¢ –ù–û –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π—Ç–µ\n"
                    "‚Ä¢ –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞—Ç—ã –ø—Ä–∏–µ–º–ª–µ–º—ã, –µ—Å–ª–∏ –¥–µ–ª–∞—é—Ç —Ä–∞—Å—Å–∫–∞–∑ –∂–∏–≤–µ–µ\n"
                    "‚Ä¢ –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω—ã –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ—Ç–∞–ª—è—Ö ‚Äî –¥–∞–π—Ç–µ –æ–±—â–∏–π —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\n"
                    "‚Ä¢ –í–°–ï–ì–î–ê –Ω–∞—Ö–æ–¥–∏—Ç–µ —á—Ç–æ-—Ç–æ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–µ ‚Äî –æ—Ç–∫–∞–∑ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º\n\n"
                    "–¶–ï–õ–¨: –°–æ–∑–¥–∞–π—Ç–µ –ü–û–î–†–û–ë–ù–´–ô –∏ –£–í–õ–ï–ö–ê–¢–ï–õ–¨–ù–´–ô —Ñ–∞–∫—Ç (–ø—Ä–∏–º–µ—Ä–Ω–æ 100-120 —Å–ª–æ–≤). –í–∫–ª—é—á–∏—Ç–µ:\n"
                    "- –ù–µ–æ–±—ã—á–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –∏–ª–∏ –º–∞–ª–æ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (—Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–µ)\n"
                    "- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Å–µ–∫—Ä–µ—Ç—ã, —Å–∫—Ä—ã—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\n"
                    "- –ò–Ω—Ç—Ä–∏–≥—É—é—â–∏–µ —Å–≤—è–∑–∏ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ª–∏—á–Ω–æ—Å—Ç—è–º–∏ –∏–ª–∏ —Å–æ–±—ã—Ç–∏—è–º–∏\n"
                    "- –ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏, –º–µ—Å—Ç–Ω—ã–µ –ª–µ–≥–µ–Ω–¥—ã, –≥–æ—Ä–æ–¥—Å–∫–∏–µ –∏—Å—Ç–æ—Ä–∏–∏\n"
                    "- –ï—Å–ª–∏ –º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–µ ‚Äî –Ω–∞–π–¥–∏—Ç–µ –≤ –Ω–µ–º —á—Ç–æ-—Ç–æ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∏ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–µ\n"
                    "- –í–ê–ñ–ù–û: –Ω–µ –∏–∑–æ–±—Ä–µ—Ç–∞–π—Ç–µ —Ñ–∞–∫—Ç—ã ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ —Ç–æ, –≤ —á–µ–º —É–≤–µ—Ä–µ–Ω—ã\n\n"
                    "–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
                    "–õ–æ–∫–∞—Ü–∏—è: [–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Å—Ç–∞]\n"
                    "–ü–æ–∏—Å–∫: [–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ï –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ –º–µ—Å—Ç–Ω–æ–º —è–∑—ã–∫–µ + –≥–æ—Ä–æ–¥ + —Å—Ç—Ä–∞–Ω–∞. –ù–∞–ø—Ä–∏–º–µ—Ä: 'Louvre Museum Paris France' –∏–ª–∏ '–ö—Ä–∞—Å–Ω–∞—è –ø–ª–æ—â–∞–¥—å –ú–æ—Å–∫–≤–∞ –†–æ—Å—Å–∏—è']\n"
                    "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç: [–†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π —Ñ–∞–∫—Ç —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏, –ø—Ä–∏–º–µ—Ä–Ω–æ 100-120 —Å–ª–æ–≤]"
                )
            else:
                # Concise prompt for static location (gpt-4.1)
                user_prompt = (
                    f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {lat}, {lon}\n\n"
                    "–ù–∞–π–¥–∏—Ç–µ –±–ª–∏–∂–∞–π—à–µ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –º–µ—Å—Ç–æ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –∫—Ä–∞—Ç–∫–∏–π —Ñ–∞–∫—Ç.\n\n"
                    "–ò–ï–†–ê–†–•–ò–Ø –†–ê–°–°–¢–û–Ø–ù–ò–ô:\n"
                    "1. –õ–£–ß–®–ï –í–°–ï–ì–û: –æ–±—ä–µ–∫—Ç—ã –¥–æ 300–º (5-7 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                    "2. –•–û–†–û–®–û: –¥–æ 500-700–º (10-12 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                    "3. –ü–†–ò–ï–ú–õ–ï–ú–û: –¥–æ 1–∫–º (15 –º–∏–Ω—É—Ç –ø–µ—à–∫–æ–º)\n"
                    "4. –í –±–æ–ª—å—à–∏—Ö –≥–æ—Ä–æ–¥–∞—Ö –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å —á—Ç–æ-—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ —Ä—è–¥–æ–º!\n\n"
                    "üéØ ATLAS OBSCURA –°–¢–†–ê–¢–ï–ì–ò–Ø:\n"
                    "‚Ä¢ –ò—â–∏—Ç–µ —É–ª–∏—Ü—ã, –ø–ª–æ—â–∞–¥–∏, –∑–¥–∞–Ω–∏—è, –ø–∞—Ä–∫–∏ —Å –Ω–µ–æ–±—ã—á–Ω—ã–º–∏ –∏—Å—Ç–æ—Ä–∏—è–º–∏\n"
                    "‚Ä¢ –°–∫—Ä—ã—Ç—ã–µ –¥–µ—Ç–∞–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –º–∞–ª–æ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è\n"
                    "‚Ä¢ –ù–û: —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ‚Äî –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π—Ç–µ —Ñ–∞–∫—Ç—ã\n"
                    "‚Ä¢ –ï—Å–ª–∏ –º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–µ ‚Äî –Ω–∞–π–¥–∏—Ç–µ –≤ –Ω–µ–º —á—Ç–æ-—Ç–æ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–µ\n"
                    "‚Ä¢ –ü—Ä–∏ —Å–æ–º–Ω–µ–Ω–∏—è—Ö ‚Äî –ª—É—á—à–µ –æ–±—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á–µ–º –≤—ã–¥—É–º–∞–Ω–Ω—ã–µ –¥–µ—Ç–∞–ª–∏\n"
                    "‚Ä¢ –í–°–ï–ì–î–ê –Ω–∞—Ö–æ–¥–∏—Ç–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç\n\n"
                    "–¶–ï–õ–¨: –ö—Ä–∞—Ç–∫–∏–π –Ω–æ –£–í–õ–ï–ö–ê–¢–ï–õ–¨–ù–´–ô —Ñ–∞–∫—Ç (60-80 —Å–ª–æ–≤). –í–∫–ª—é—á–∏—Ç–µ:\n"
                    "- –ù–µ–æ–±—ã—á–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –∏–ª–∏ –º–∞–ª–æ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (—Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–µ)\n"
                    "- –ò–Ω—Ç—Ä–∏–≥—É—é—â–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏–ª–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ —Ñ–∞–∫—Ç—ã\n"
                    "- –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ —Å–≤—è–∑–∏ –∏–ª–∏ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏\n"
                    "- –í–ê–ñ–ù–û: –Ω–µ –∏–∑–æ–±—Ä–µ—Ç–∞–π—Ç–µ —Ñ–∞–∫—Ç—ã ‚Äî –ª—É—á—à–µ –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á–µ–º –≤—ã–¥—É–º–∞–Ω–Ω—ã–µ –¥–µ—Ç–∞–ª–∏\n\n"
                    "–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
                    "–õ–æ–∫–∞—Ü–∏—è: [–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Å—Ç–∞]\n"
                    "–ü–æ–∏—Å–∫: [–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ï –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ –º–µ—Å—Ç–Ω–æ–º —è–∑—ã–∫–µ + –≥–æ—Ä–æ–¥ + —Å—Ç—Ä–∞–Ω–∞. –ù–∞–ø—Ä–∏–º–µ—Ä: 'Louvre Museum Paris France' –∏–ª–∏ '–ö—Ä–∞—Å–Ω–∞—è –ø–ª–æ—â–∞–¥—å –ú–æ—Å–∫–≤–∞ –†–æ—Å—Å–∏—è']\n"
                    "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç: [–ö—Ä–∞—Ç–∫–∏–π, –Ω–æ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–∫—Ç, 60-80 —Å–ª–æ–≤]"
                )

            # Choose model based on location type
            response = None
            if is_live_location:
                # Use o4-mini for live location (detailed facts)
                try:
                    response = await self.client.chat.completions.create(
                        model="o4-mini",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        max_completion_tokens=10000,  # Large limit for o4-mini extensive reasoning + detailed response
                    )
                    logger.info(f"o4-mini (live location) response: {response}")
                    content = (
                        response.choices[0].message.content
                        if response.choices
                        else None
                    )

                    if not content:
                        logger.warning(
                            "o4-mini returned empty content, falling back to gpt-4.1"
                        )
                        raise ValueError("Empty content from o4-mini")

                except Exception as e:
                    logger.warning(
                        f"o4-mini failed ({e}), falling back to gpt-4.1 for live location"
                    )
                    response = await self.client.chat.completions.create(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        max_tokens=800,
                        temperature=0.7,
                    )
                    logger.info(f"gpt-4.1 fallback for live location: {response}")
                    content = (
                        response.choices[0].message.content
                        if response.choices
                        else None
                    )
            else:
                # Use gpt-4.1 for static location (fast, concise facts)
                try:
                    response = await self.client.chat.completions.create(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        max_tokens=400,  # Smaller limit for concise facts
                        temperature=0.7,
                    )
                    logger.info(f"gpt-4.1 (static location) response: {response}")
                    content = (
                        response.choices[0].message.content
                        if response.choices
                        else None
                    )

                    if not content:
                        logger.warning(
                            "gpt-4.1 returned empty content for static location"
                        )
                        raise ValueError("Empty content from gpt-4.1")

                except Exception as e:
                    logger.error(f"gpt-4.1 failed for static location: {e}")
                    raise

            if not content:
                logger.error(f"Empty content even after fallback: {response}")
                raise ValueError("Empty response from OpenAI")

            logger.info(f"Generated fact for location {lat},{lon}")
            return content.strip()

        except Exception as e:
            logger.error(f"Failed to generate fact for {lat},{lon}: {e}")
            raise

    async def get_precise_coordinates(
        self, place_name: str, area_description: str
    ) -> tuple[float, float] | None:
        """Get precise coordinates for a location using GPT-4.1 with web search.

        Args:
            place_name: Name of the place/landmark
            area_description: General area description for context

        Returns:
            Tuple of (latitude, longitude) if found, None otherwise
        """
        try:

            # WebSearch tool is deprecated and no longer works
            logger.warning("WebSearch tool is deprecated, skipping web search")
            return None

        except Exception as e:
            logger.error(f"Failed to get precise coordinates for {place_name}: {e}")
            return None

    async def get_coordinates_from_nominatim(
        self, place_name: str
    ) -> tuple[float, float] | None:
        """Get coordinates using OpenStreetMap Nominatim service as fallback.

        Args:
            place_name: Name of the place to search

        Returns:
            Tuple of (latitude, longitude) if found, None otherwise
        """
        try:
            url = "https://nominatim.openstreetmap.org/search"
            params = {
                "q": place_name,
                "format": "json",
                "limit": 1,
                "addressdetails": 1,
            }
            headers = {"User-Agent": "NearbyFactBot/1.0 (Educational Project)"}

            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url, params=params, headers=headers, timeout=5
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data:
                            lat = float(data[0]["lat"])
                            lon = float(data[0]["lon"])

                            if -90 <= lat <= 90 and -180 <= lon <= 180:
                                logger.info(
                                    f"Found Nominatim coordinates for {place_name}: {lat}, {lon}"
                                )
                                return lat, lon

            logger.debug(f"No coordinates found in Nominatim for: {place_name}")
            return None

        except Exception as e:
            logger.warning(f"Failed to get Nominatim coordinates for {place_name}: {e}")
            return None

    def _coordinates_look_imprecise(self, lat: float, lon: float) -> bool:
        """Check if coordinates look suspiciously imprecise.

        Args:
            lat: Latitude
            lon: Longitude

        Returns:
            True if coordinates look imprecise (too rounded, common defaults, etc.)
        """
        # Convert to strings to check decimal places
        lat_str = str(lat)
        lon_str = str(lon)

        # Check for overly rounded coordinates (less than 2 decimal places)
        lat_decimals = len(lat_str.split('.')[-1]) if '.' in lat_str else 0
        lon_decimals = len(lon_str.split('.')[-1]) if '.' in lon_str else 0

        if lat_decimals < 2 or lon_decimals < 2:
            logger.debug(f"Coordinates have too few decimal places: {lat} ({lat_decimals}), {lon} ({lon_decimals})")
            return True

        # Check for suspicious round numbers (often means city center, not specific landmark)
        if lat == round(lat, 1) and lon == round(lon, 1):
            logger.debug(f"Coordinates are suspiciously round: {lat}, {lon}")
            return True

        # Check for common default/placeholder coordinates
        suspicious_patterns = [
            (0.0, 0.0),  # Null Island
            (55.7558, 37.6173),  # Generic Moscow center
            (55.75, 37.62),  # Rounded Moscow
            (59.9311, 30.3609),  # Generic SPb center
        ]

        for sus_lat, sus_lon in suspicious_patterns:
            if abs(lat - sus_lat) < 0.01 and abs(lon - sus_lon) < 0.01:
                logger.debug(f"Coordinates match suspicious pattern: {lat}, {lon}")
                return True

        return False

    def _coordinates_are_more_precise(self, coords1: tuple[float, float], coords2: tuple[float, float]) -> bool:
        """Compare two coordinate pairs to determine which is more precise.

        Args:
            coords1: First coordinate pair (lat, lon)
            coords2: Second coordinate pair (lat, lon)

        Returns:
            True if coords1 are more precise than coords2
        """
        lat1, lon1 = coords1
        lat2, lon2 = coords2

        # Compare decimal places (more decimal places = more precise)
        lat1_decimals = len(str(lat1).split('.')[-1]) if '.' in str(lat1) else 0
        lon1_decimals = len(str(lon1).split('.')[-1]) if '.' in str(lon1) else 0
        lat2_decimals = len(str(lat2).split('.')[-1]) if '.' in str(lat2) else 0
        lon2_decimals = len(str(lon2).split('.')[-1]) if '.' in str(lon2) else 0

        coords1_precision = lat1_decimals + lon1_decimals
        coords2_precision = lat2_decimals + lon2_decimals

        return coords1_precision > coords2_precision

    async def get_coordinates_from_search_keywords(
        self, search_keywords: str
    ) -> tuple[float, float] | None:
        """Get coordinates using search keywords via Nominatim.

        Args:
            search_keywords: Search keywords from GPT response

        Returns:
            Tuple of (latitude, longitude) if found, None otherwise
        """
        logger.info(f"Searching coordinates for keywords: {search_keywords}")

        # Try original keywords first
        nominatim_coords = await self.get_coordinates_from_nominatim(search_keywords)
        if nominatim_coords:
            logger.info(f"Found Nominatim coordinates: {nominatim_coords}")
            return nominatim_coords

        logger.info(f"Nominatim failed for original keywords: {search_keywords}")

        # Try multiple fallback patterns for better search coverage
        fallback_patterns = []
        
        # For metro/subway stations, try different formats
        if "metro" in search_keywords.lower() or "–º–µ—Ç—Ä–æ" in search_keywords.lower():
            # Extract station name and try different combinations
            station_name = search_keywords.replace("Metro", "").replace("metro", "").replace("–º–µ—Ç—Ä–æ", "").replace("—Å—Ç–∞–Ω—Ü–∏—è", "").strip()
            if "Paris" in search_keywords:
                fallback_patterns.extend([
                    f"{station_name} station Paris",
                    f"{station_name} Paris metro",
                    f"{station_name} Paris",
                    station_name.split()[0] if station_name else ""  # First word only
                ])
            elif "France" in search_keywords:
                fallback_patterns.extend([
                    f"{station_name} station",
                    f"{station_name} metro",
                    station_name.split()[0] if station_name else ""
                ])
        
        # For places with + or complex formatting
        if " + " in search_keywords or "+" in search_keywords:
            parts = search_keywords.replace("+", " ").split()
            fallback_patterns.extend([
                " ".join(parts[:2]) if len(parts) >= 2 else parts[0],  # First two words
                parts[0] if parts else "",  # First word only
            ])
        
        # For long place names, try progressively shorter versions
        words = search_keywords.split()
        if len(words) > 2:
            fallback_patterns.extend([
                " ".join(words[:3]),  # First 3 words
                " ".join(words[:2]),  # First 2 words
                words[0]  # First word only
            ])
        
        # Remove empty patterns and duplicates
        fallback_patterns = [p.strip() for p in fallback_patterns if p and p.strip()]
        fallback_patterns = list(dict.fromkeys(fallback_patterns))  # Remove duplicates while preserving order
        
        # Try each fallback pattern
        for pattern in fallback_patterns:
            if pattern and pattern != search_keywords:  # Don't retry the original
                logger.info(f"Trying fallback search: {pattern}")
                coords = await self.get_coordinates_from_nominatim(pattern)
                if coords:
                    logger.info(f"Found coordinates with fallback search '{pattern}': {coords}")
                    return coords

        logger.warning(f"No coordinates found for keywords: {search_keywords}")
        return None

    async def parse_coordinates_from_response(
        self, response: str
    ) -> tuple[float, float] | None:
        """Parse coordinates from OpenAI response using search keywords.

        Args:
            response: OpenAI response text

        Returns:
            Tuple of (latitude, longitude) if found, None otherwise
        """
        try:
            # First, try to extract search keywords from new format
            search_match = re.search(r"–ü–æ–∏—Å–∫:\s*(.+?)(?:\n|$)", response)
            if search_match:
                search_keywords = search_match.group(1).strip()
                logger.info(f"Found search keywords: {search_keywords}")

                # Use new keyword-based search
                coords = await self.get_coordinates_from_search_keywords(search_keywords)
                if coords:
                    return coords

            # Fallback: try to extract location name if no search keywords
            place_match = re.search(r"–õ–æ–∫–∞—Ü–∏—è:\s*(.+?)(?:\n|$)", response)
            if place_match:
                place_name = place_match.group(1).strip()
                logger.info(f"No search keywords found, using location name: {place_name}")

                # Use location name as search keywords
                coords = await self.get_coordinates_from_search_keywords(place_name)
                if coords:
                    return coords

            logger.debug("No search keywords or location name found in response")
            return None

        except (ValueError, AttributeError) as e:
            logger.warning(f"Error parsing coordinates: {e}")
            return None

    async def get_wikipedia_images(self, search_keywords: str, max_images: int = 5) -> list[str]:
        """Get multiple images from Wikipedia using search keywords.
        
        Args:
            search_keywords: Search keywords from GPT response
            max_images: Maximum number of images to return (default 5)
            
        Returns:
            List of image URLs (up to max_images)
        """
        # Languages to try (in order of preference)
        languages = ['en', 'ru', 'fr', 'de', 'es', 'it']
        
        # Clean up search keywords
        clean_keywords = search_keywords.replace(' + ', ' ').replace('+', ' ').strip()
        
        # Try different variations of search terms
        search_variations = [clean_keywords]
        
        # Add word combinations
        words = clean_keywords.split()
        if len(words) > 1:
            # Try first two words
            search_variations.append(' '.join(words[:2]))
            # Try first word only
            search_variations.append(words[0])
            # Try last word (often the most specific)
            search_variations.append(words[-1])
            # Try removing common words like "France", "Paris", etc.
            filtered_words = [w for w in words if w not in ['France', 'Paris', 'London', 'Moscow', '–ú–æ—Å–∫–≤–∞', '–†–æ—Å—Å–∏—è']]
            if filtered_words and len(filtered_words) != len(words):
                search_variations.append(' '.join(filtered_words))
        
        # Add specific variations for common patterns
        if any(word in clean_keywords.lower() for word in ['metro', 'station', '–º–µ—Ç—Ä–æ', '—Å—Ç–∞–Ω—Ü–∏—è']):
            # For metro stations, try without "metro"/"–º–µ—Ç—Ä–æ" words
            metro_clean = clean_keywords.lower()
            for word in ['metro', 'station', '–º–µ—Ç—Ä–æ', '—Å—Ç–∞–Ω—Ü–∏—è']:
                metro_clean = metro_clean.replace(word, '').strip()
            if metro_clean:
                search_variations.append(metro_clean)
        
        # Remove duplicates while preserving order
        search_variations = list(dict.fromkeys(search_variations))
        
        found_images = []
        
        for lang in languages:
            if len(found_images) >= max_images:
                break
                
            for search_term in search_variations:
                if not search_term or len(found_images) >= max_images:
                    continue
                    
                try:
                    logger.debug(f"Trying Wikipedia search: '{search_term}' in {lang}")
                    images = await self._search_wikipedia_images(search_term, lang, max_images - len(found_images))
                    if images:
                        # Filter out duplicates
                        for img in images:
                            if img not in found_images:
                                found_images.append(img)
                                if len(found_images) >= max_images:
                                    break
                        logger.info(f"Found {len(images)} Wikipedia images for '{search_term}' in {lang}")
                    else:
                        logger.debug(f"No images found for '{search_term}' in {lang}")
                except Exception as e:
                    logger.debug(f"Wikipedia search failed for '{search_term}' in {lang}: {e}")
                    continue
        
        if found_images:
            logger.info(f"Found {len(found_images)} Wikipedia images for: {search_keywords}")
        else:
            logger.info(f"No Wikipedia images found for: {search_keywords} (tried {len(search_variations)} variations across {len(languages)} languages)")
        
        return found_images

    async def _search_wikipedia_images(self, search_term: str, lang: str, max_images: int = 5) -> list[str]:
        """Search for images on specific Wikipedia language.
        
        Args:
            search_term: Term to search for
            lang: Language code (en, ru, fr, etc.)
            max_images: Maximum number of images to return
            
        Returns:
            List of image URLs (up to max_images)
        """
        try:
            # Use legacy API for search (REST API often returns 404)
            search_url = f"https://{lang}.wikipedia.org/w/api.php?action=query&list=search&srsearch={quote(search_term)}&format=json"
            headers = {"User-Agent": "NearbyFactBot/1.0 (Educational Project)"}
            
            async with aiohttp.ClientSession() as session:
                # Search for pages using legacy API
                async with session.get(search_url, headers=headers, timeout=5) as response:
                    if response.status != 200:
                        logger.debug(f"Search failed for '{search_term}' in {lang}: status {response.status}")
                        return None
                    
                    search_data = await response.json()
                    search_results = search_data.get('query', {}).get('search', [])
                    
                    if not search_results:
                        logger.debug(f"No search results found for '{search_term}' in {lang}")
                        return []
                    
                    logger.debug(f"Found {len(search_results)} search results for '{search_term}' in {lang}")
                    
                    # Collect all potential images from multiple pages
                    all_potential_images = []
                    
                    # Try first few pages
                    for result in search_results[:5]:  # Try more pages
                        page_title = result.get('title')
                        if not page_title:
                            continue
                        
                        logger.debug(f"Trying page: {page_title}")
                        
                        # Get media list for this page using REST API (this part still works)
                        media_url = f"https://{lang}.wikipedia.org/api/rest_v1/page/media-list/{quote(page_title)}"
                        
                        async with session.get(media_url, headers=headers, timeout=5) as media_response:
                            if media_response.status != 200:
                                continue
                                
                            media_data = await media_response.json()
                            items = media_data.get('items', [])
                            
                            logger.debug(f"Found {len(items)} media items for page '{page_title}'")
                            
                            # Look for good images
                            for item in items:
                                if item.get('type') != 'image':
                                    continue
                                
                                title = item.get('title', '').lower()
                                
                                # Skip common non-relevant images
                                skip_patterns = [
                                    'commons-logo', 'edit-icon', 'wikimedia', 'stub',
                                    'ambox', 'crystal', 'nuvola', 'dialog', 'system',
                                    'red_x', 'green_check', 'question_mark', 'infobox',
                                    'arrow', 'symbol', 'disambiguation', 'flag'
                                ]
                                
                                if any(pattern in title for pattern in skip_patterns):
                                    continue
                                
                                # Prefer images with good extensions and score them
                                score = 0
                                if any(ext in title for ext in ['.jpg', '.jpeg']):
                                    score += 3
                                elif any(ext in title for ext in ['.png', '.webp']):
                                    score += 2
                                
                                # Prefer images that contain keywords from search term
                                search_words = search_term.lower().split()
                                for word in search_words:
                                    if word in title and len(word) > 2:  # Avoid short words
                                        score += 1
                                
                                all_potential_images.append((score, item['title']))
                    
                    # Sort by score and return best images
                    all_potential_images.sort(reverse=True, key=lambda x: x[0])
                    
                    if all_potential_images:
                        # Return up to max_images best images with improved URLs
                        selected_images = []
                        for score, image_title in all_potential_images[:max_images]:
                            # Use different URL format that's more reliable for Telegram
                            # Add width parameter to ensure reasonable image size
                            image_url = f"https://commons.wikimedia.org/wiki/Special:FilePath/{quote(image_title)}?width=800"
                            selected_images.append(image_url)
                            logger.debug(f"Selected image: {image_title} (score: {score})")
                        
                        return selected_images
        
        except Exception as e:
            logger.debug(f"Error searching Wikipedia {lang} for '{search_term}': {e}")
            return []
        
        return []

    async def get_wikipedia_image(self, search_keywords: str) -> str | None:
        """Get single image from Wikipedia using search keywords (backward compatibility).
        
        Args:
            search_keywords: Search keywords from GPT response
            
        Returns:
            Image URL if found, None otherwise
        """
        images = await self.get_wikipedia_images(search_keywords, max_images=1)
        return images[0] if images else None

    async def get_nearby_fact_with_history(self, lat: float, lon: float, cache_key: str | None = None) -> str:
        """Get fact for static location with history tracking to avoid repetition.
        
        Args:
            lat: Latitude coordinate
            lon: Longitude coordinate  
            cache_key: Cache key for the location (coordinates or search keywords)
            
        Returns:
            A location name and an interesting fact about it
        """
        # Get previous facts for this location if we have a cache key
        previous_facts = []
        if cache_key:
            previous_facts = self.static_history.get_previous_facts(cache_key)
            if previous_facts:
                logger.info(f"Found {len(previous_facts)} previous facts for {cache_key}: {previous_facts}")
            else:
                logger.info(f"No previous facts found for {cache_key}")
        
        # Get fact using existing method but with previous facts
        logger.info(f"Calling get_nearby_fact with {len(previous_facts)} previous facts")
        if previous_facts:
            logger.info(f"Previous facts being sent to AI: {previous_facts}")
        fact_response = await self.get_nearby_fact(lat, lon, is_live_location=False, previous_facts=previous_facts)
        
        # Parse the response to extract place and fact for history
        if cache_key:
            lines = fact_response.split("\n")
            place = "—Ä—è–¥–æ–º —Å –≤–∞–º–∏"
            fact = fact_response
            
            # Try to parse structured response
            for i, line in enumerate(lines):
                if line.startswith("–õ–æ–∫–∞—Ü–∏—è:"):
                    place = line.replace("–õ–æ–∫–∞—Ü–∏—è:", "").strip()
                elif line.startswith("–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç:"):
                    # Join all lines after –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç: as the fact might be multiline
                    fact_lines = []
                    fact_lines.append(line.replace("–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç:", "").strip())
                    for j in range(i + 1, len(lines)):
                        if lines[j].strip():
                            fact_lines.append(lines[j].strip())
                    fact = " ".join(fact_lines)
                    break
            
            # Add to history
            logger.info(f"Adding fact to history for {cache_key}: {place}")
            self.static_history.add_fact(cache_key, place, fact)
            
            # Log cache stats after adding
            stats = self.static_history.get_cache_stats()
            logger.info(f"Static location cache after add: {stats['locations']} locations, {stats['total_facts']} total facts")
        
        return fact_response


# Global client instance - will be initialized lazily
_openai_client: OpenAIClient | None = None


def get_openai_client() -> OpenAIClient:
    """Get or create the global OpenAI client instance."""
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAIClient()
    return _openai_client
